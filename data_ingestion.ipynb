{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro dlt -> LanceDB loading\n",
    "\n",
    "## Install requirements\n",
    "\n",
    "To create a json -> lancedb pipeline, we need to install:\n",
    "1. dlt with lancedb extras\n",
    "2. sentence-transformers: we need to use an embedding model to vectorize and store data inside LanceDB. For this we choose the open-source model \n",
    "<a href=\"https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#model-overview\">sentence-transformers/all-MiniLM-L6-v2</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: dlt[lancedb]==0.5.1a0\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m410.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m436.3 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from sentence-transformers)\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.3.1-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: numpy in /Users/Aaron/anaconda3/envs/py312_env/lib/python3.12/site-packages (from sentence-transformers) (1.26.4)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.5.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: scipy in /Users/Aaron/anaconda3/envs/py312_env/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Collecting huggingface-hub>=0.15.1 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: Pillow in /Users/Aaron/anaconda3/envs/py312_env/lib/python3.12/site-packages (from sentence-transformers) (10.3.0)\n",
      "Collecting filelock (from huggingface-hub>=0.15.1->sentence-transformers)\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.15.1->sentence-transformers)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/Aaron/anaconda3/envs/py312_env/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/Aaron/anaconda3/envs/py312_env/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/Aaron/anaconda3/envs/py312_env/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/Aaron/anaconda3/envs/py312_env/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
      "Collecting sympy (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.13.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/Aaron/anaconda3/envs/py312_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Downloading regex-2024.5.15-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Downloading safetensors-0.4.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/Aaron/anaconda3/envs/py312_env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/Aaron/anaconda3/envs/py312_env/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Aaron/anaconda3/envs/py312_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Aaron/anaconda3/envs/py312_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Aaron/anaconda3/envs/py312_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Aaron/anaconda3/envs/py312_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.6.2)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "Downloading huggingface_hub-0.23.5-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.8/402.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.3.1-cp312-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 MB\u001b[0m \u001b[31m608.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m721.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.1-cp312-cp312-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m625.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Downloading regex-2024.5.15-cp312-cp312-macosx_11_0_arm64.whl (278 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.5/278.5 kB\u001b[0m \u001b[31m895.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp312-cp312-macosx_11_0_arm64.whl (411 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m769.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m642.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.0-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m684.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, tqdm, threadpoolctl, sympy, safetensors, regex, networkx, fsspec, filelock, torch, scikit-learn, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed filelock-3.15.4 fsspec-2024.6.1 huggingface-hub-0.23.5 mpmath-1.3.0 networkx-3.3 regex-2024.5.15 safetensors-0.4.3 scikit-learn-1.5.1 sentence-transformers-3.0.1 sympy-1.13.0 threadpoolctl-3.5.0 tokenizers-0.19.1 torch-2.3.1 tqdm-4.66.4 transformers-4.42.4\n"
     ]
    }
   ],
   "source": [
    "!pip install dlt[lancedb]==0.5.1a0\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "We'll first load the data just into LanceDB, without embedding it. LanceDB stores both the data and the embeddings, and can also embed data and queries on the fly.\n",
    "\n",
    "Some definitions:\n",
    "* A dlt **source** is a grouping of **resources** (e.g. all your data from Hubspot)\n",
    "* A dlt **resource** is a function that yields data (e.g. a function yielding all your Hubspot companies)\n",
    "* A dlt **pipeline** is how you ingest your data\n",
    "\n",
    "Loading the data consists of a few steps:\n",
    "1. Use the requests library to get the data\n",
    "2. Define a dlt resource that yields the individual documents\n",
    "3. Create a dlt pipeline and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import dlt\n",
    "import os\n",
    "from dlt.destinations.adapters import lancedb_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Aaron/anaconda3/envs/py312_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_dlt_pipeline_state\n",
      "[{'name': 'version', 'data_type': 'bigint', 'nullable': False}, {'name': 'engine_version', 'data_type': 'bigint', 'nullable': False}, {'name': 'pipeline_name', 'data_type': 'text', 'nullable': False}, {'name': 'state', 'data_type': 'text', 'nullable': False}, {'name': 'created_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'version_hash', 'data_type': 'text', 'nullable': True}, {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True}]\n",
      "documents\n",
      "[{'name': 'text', 'data_type': 'text', 'nullable': True}, {'name': 'section', 'data_type': 'text', 'nullable': True}, {'name': 'question', 'data_type': 'text', 'nullable': True}, {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True}]\n",
      "_dlt_loads\n",
      "[{'name': 'load_id', 'data_type': 'text', 'nullable': False}, {'name': 'schema_name', 'data_type': 'text', 'nullable': True}, {'name': 'status', 'data_type': 'bigint', 'nullable': False}, {'name': 'inserted_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'schema_version_hash', 'data_type': 'text', 'nullable': True}]\n",
      "_dlt_version\n",
      "[{'name': 'version', 'data_type': 'bigint', 'nullable': False}, {'name': 'engine_version', 'data_type': 'bigint', 'nullable': False}, {'name': 'inserted_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'schema_name', 'data_type': 'text', 'nullable': False}, {'name': 'version_hash', 'data_type': 'text', 'nullable': False}, {'name': 'schema', 'data_type': 'text', 'nullable': False}]\n",
      "UPLOAD\n",
      "Pipeline from_json load step completed in 0.22 seconds\n",
      "1 load package(s) were loaded to destination LanceDB and into dataset qanda\n",
      "The LanceDB destination used <dlt.destinations.impl.lancedb.configuration.LanceDBCredentials object at 0x12eb1c920> location to store data\n",
      "Load package 1721141700.70987 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "qa_dataset = requests.get(\"https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1\").json()\n",
    "\n",
    "@dlt.resource\n",
    "def qa_documents():\n",
    "  for course in qa_dataset:\n",
    "    yield course[\"documents\"]\n",
    "\n",
    "pipeline = dlt.pipeline(pipeline_name=\"from_json\", destination=\"lancedb\", dataset_name=\"qanda\")\n",
    "\n",
    "load_info = pipeline.run(qa_documents, table_name=\"documents\")\n",
    "\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qanda____dlt_loads', 'qanda____dlt_pipeline_state', 'qanda____dlt_version', 'qanda___dltSentinelTable', 'qanda___documents']\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "db = lancedb.connect(\"./.lancedb\")\n",
    "print(db.table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id__</th>\n",
       "      <th>text</th>\n",
       "      <th>section</th>\n",
       "      <th>question</th>\n",
       "      <th>_dlt_load_id</th>\n",
       "      <th>_dlt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2da55a48-f9c2-5212-b807-29d0ff23a3d6</td>\n",
       "      <td>The purpose of this document is to capture fre...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - When will the course start?</td>\n",
       "      <td>1721141700.70987</td>\n",
       "      <td>XMR/HNq1mLbm5Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89a9b796-d8c3-5cb6-98d6-769ae87308b9</td>\n",
       "      <td>GitHub - DataTalksClub data-engineering-zoomca...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - What are the prerequisites for this c...</td>\n",
       "      <td>1721141700.70987</td>\n",
       "      <td>GQig4sLK5gDGCg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c69a8fdc-b512-510a-8c2c-3881e32a6b3d</td>\n",
       "      <td>Yes, even if you don't register, you're still ...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - Can I still join the course after the...</td>\n",
       "      <td>1721141700.70987</td>\n",
       "      <td>eIlgFythKCDITA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aec43c4c-4dac-524e-b6fe-01b6c57c60ba</td>\n",
       "      <td>You don't need it. You're accepted. You can al...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - I have registered for the Data Engine...</td>\n",
       "      <td>1721141700.70987</td>\n",
       "      <td>U4tYyeNopoGztg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67d1fee7-9c7b-53ef-801b-eaad216e1fba</td>\n",
       "      <td>You can start by installing and setting up all...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - What can I do before the course starts?</td>\n",
       "      <td>1721141700.70987</td>\n",
       "      <td>FV0FSrLd75cjvA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>20a8de85-4496-56d9-86df-9ce333fc081e</td>\n",
       "      <td>Problem description\\nThis is the step in the c...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Github actions: Permission denied error when e...</td>\n",
       "      <td>1721141700.70987</td>\n",
       "      <td>sXIiR9Cuw6WQCQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>c3919fa6-8c72-55f4-bcd2-f428494b49a4</td>\n",
       "      <td>Problem description\\nWhen a docker-compose fil...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Managing Multiple Docker Containers with docke...</td>\n",
       "      <td>1721141700.70987</td>\n",
       "      <td>kHDeLJcs0+3u1w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>31343233-bc09-57f1-822e-f3fd9a4a50d4</td>\n",
       "      <td>Problem description\\nIf you are having problem...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>AWS regions need to match docker-compose</td>\n",
       "      <td>1721141700.70987</td>\n",
       "      <td>Kwn0NXFaRNJvcQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>1e75631d-b41f-5b01-b036-cf10b80f8192</td>\n",
       "      <td>Problem description\\nPre-commit command was fa...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Isort Pre-commit</td>\n",
       "      <td>1721141700.70987</td>\n",
       "      <td>IiNMgj++J1mqzQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>5f95bc64-0972-50c4-a7b3-a32bfb45f64f</td>\n",
       "      <td>Problem description\\nInfrastructure created in...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>How to destroy infrastructure created via GitH...</td>\n",
       "      <td>1721141700.70987</td>\n",
       "      <td>uhK2ZLwilpQ2NQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>948 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id__  \\\n",
       "0    2da55a48-f9c2-5212-b807-29d0ff23a3d6   \n",
       "1    89a9b796-d8c3-5cb6-98d6-769ae87308b9   \n",
       "2    c69a8fdc-b512-510a-8c2c-3881e32a6b3d   \n",
       "3    aec43c4c-4dac-524e-b6fe-01b6c57c60ba   \n",
       "4    67d1fee7-9c7b-53ef-801b-eaad216e1fba   \n",
       "..                                    ...   \n",
       "943  20a8de85-4496-56d9-86df-9ce333fc081e   \n",
       "944  c3919fa6-8c72-55f4-bcd2-f428494b49a4   \n",
       "945  31343233-bc09-57f1-822e-f3fd9a4a50d4   \n",
       "946  1e75631d-b41f-5b01-b036-cf10b80f8192   \n",
       "947  5f95bc64-0972-50c4-a7b3-a32bfb45f64f   \n",
       "\n",
       "                                                  text  \\\n",
       "0    The purpose of this document is to capture fre...   \n",
       "1    GitHub - DataTalksClub data-engineering-zoomca...   \n",
       "2    Yes, even if you don't register, you're still ...   \n",
       "3    You don't need it. You're accepted. You can al...   \n",
       "4    You can start by installing and setting up all...   \n",
       "..                                                 ...   \n",
       "943  Problem description\\nThis is the step in the c...   \n",
       "944  Problem description\\nWhen a docker-compose fil...   \n",
       "945  Problem description\\nIf you are having problem...   \n",
       "946  Problem description\\nPre-commit command was fa...   \n",
       "947  Problem description\\nInfrastructure created in...   \n",
       "\n",
       "                              section  \\\n",
       "0    General course-related questions   \n",
       "1    General course-related questions   \n",
       "2    General course-related questions   \n",
       "3    General course-related questions   \n",
       "4    General course-related questions   \n",
       "..                                ...   \n",
       "943          Module 6: Best practices   \n",
       "944          Module 6: Best practices   \n",
       "945          Module 6: Best practices   \n",
       "946          Module 6: Best practices   \n",
       "947          Module 6: Best practices   \n",
       "\n",
       "                                              question      _dlt_load_id  \\\n",
       "0                 Course - When will the course start?  1721141700.70987   \n",
       "1    Course - What are the prerequisites for this c...  1721141700.70987   \n",
       "2    Course - Can I still join the course after the...  1721141700.70987   \n",
       "3    Course - I have registered for the Data Engine...  1721141700.70987   \n",
       "4     Course - What can I do before the course starts?  1721141700.70987   \n",
       "..                                                 ...               ...   \n",
       "943  Github actions: Permission denied error when e...  1721141700.70987   \n",
       "944  Managing Multiple Docker Containers with docke...  1721141700.70987   \n",
       "945           AWS regions need to match docker-compose  1721141700.70987   \n",
       "946                                   Isort Pre-commit  1721141700.70987   \n",
       "947  How to destroy infrastructure created via GitH...  1721141700.70987   \n",
       "\n",
       "            _dlt_id  \n",
       "0    XMR/HNq1mLbm5Q  \n",
       "1    GQig4sLK5gDGCg  \n",
       "2    eIlgFythKCDITA  \n",
       "3    U4tYyeNopoGztg  \n",
       "4    FV0FSrLd75cjvA  \n",
       "..              ...  \n",
       "943  sXIiR9Cuw6WQCQ  \n",
       "944  kHDeLJcs0+3u1w  \n",
       "945  Kwn0NXFaRNJvcQ  \n",
       "946  IiNMgj++J1mqzQ  \n",
       "947  uhK2ZLwilpQ2NQ  \n",
       "\n",
       "[948 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_table = db.open_table(\"qanda___documents\")\n",
    "\n",
    "db_table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and embed the data\n",
    "\n",
    "Now we load the same data again (into a new table), but embed it directly with the `lancedb_adapter`. This consists of the following steps:\n",
    "\n",
    "1. Define the embedding model to use via ENV variables\n",
    "2. Define a new pipeline to load the same data and embed the \"text\" and \"question\" columns with the `lancedb_adapter`\n",
    "\n",
    "You can use any embedding model, from open source to OpenAI. We've chosen the [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) sentence transformer for speed and simplicty.\n",
    "\n",
    "Note: this pipeline runs slightly longer because it has to download the model and embed the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_dlt_pipeline_state\n",
      "[{'name': 'version', 'data_type': 'bigint', 'nullable': False}, {'name': 'engine_version', 'data_type': 'bigint', 'nullable': False}, {'name': 'pipeline_name', 'data_type': 'text', 'nullable': False}, {'name': 'state', 'data_type': 'text', 'nullable': False}, {'name': 'created_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'version_hash', 'data_type': 'text', 'nullable': True}, {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True}]\n",
      "documents\n",
      "[{'name': 'text', 'x-lancedb-embed': True, 'data_type': 'text', 'nullable': True}, {'name': 'section', 'data_type': 'text', 'nullable': True}, {'name': 'question', 'x-lancedb-embed': True, 'data_type': 'text', 'nullable': True}, {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True}]\n",
      "_dlt_loads\n",
      "[{'name': 'load_id', 'data_type': 'text', 'nullable': False}, {'name': 'schema_name', 'data_type': 'text', 'nullable': True}, {'name': 'status', 'data_type': 'bigint', 'nullable': False}, {'name': 'inserted_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'schema_version_hash', 'data_type': 'text', 'nullable': True}]\n",
      "_dlt_version\n",
      "[{'name': 'version', 'data_type': 'bigint', 'nullable': False}, {'name': 'engine_version', 'data_type': 'bigint', 'nullable': False}, {'name': 'inserted_at', 'data_type': 'timestamp', 'nullable': False}, {'name': 'schema_name', 'data_type': 'text', 'nullable': False}, {'name': 'version_hash', 'data_type': 'text', 'nullable': False}, {'name': 'schema', 'data_type': 'text', 'nullable': False}]\n",
      "UPLOAD\n",
      "Pipeline from_json_embedded load step completed in 2 minutes and 51.31 seconds\n",
      "1 load package(s) were loaded to destination LanceDB and into dataset qanda_embedded\n",
      "The LanceDB destination used <dlt.destinations.impl.lancedb.configuration.LanceDBCredentials object at 0x12ebe1a00> location to store data\n",
      "Load package 1721153497.323018 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL_PROVIDER\"] = \"sentence-transformers\"\n",
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL\"] = \"all-MiniLM-L6-v2\"\n",
    "pipeline = dlt.pipeline(pipeline_name=\"from_json_embedded\", destination=\"lancedb\", dataset_name=\"qanda_embedded\")\n",
    "load_info = pipeline.run(lancedb_adapter(qa_documents, embed=[\"text\", \"question\"]), table_name=\"documents\")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qanda____dlt_loads', 'qanda____dlt_pipeline_state', 'qanda____dlt_version', 'qanda___dltSentinelTable', 'qanda___documents', 'qanda_embedded____dlt_loads', 'qanda_embedded____dlt_pipeline_state', 'qanda_embedded____dlt_version', 'qanda_embedded___dltSentinelTable', 'qanda_embedded___documents']\n"
     ]
    }
   ],
   "source": [
    "db = lancedb.connect(\"./.lancedb\")\n",
    "print(db.table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id__</th>\n",
       "      <th>vector__</th>\n",
       "      <th>text</th>\n",
       "      <th>section</th>\n",
       "      <th>question</th>\n",
       "      <th>_dlt_load_id</th>\n",
       "      <th>_dlt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9b6b8e02-7473-529b-8b65-5b90a44c4ccc</td>\n",
       "      <td>[-0.00035096044, -0.062014237, -0.03799987, 0....</td>\n",
       "      <td>The purpose of this document is to capture fre...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - When will the course start?</td>\n",
       "      <td>1721153497.323018</td>\n",
       "      <td>xDljAqW4Go+btw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ff3d1217-a66e-5744-8e2e-ffb044b8cd12</td>\n",
       "      <td>[0.02001144, -0.011535538, 0.013017191, -0.002...</td>\n",
       "      <td>GitHub - DataTalksClub data-engineering-zoomca...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - What are the prerequisites for this c...</td>\n",
       "      <td>1721153497.323018</td>\n",
       "      <td>lXnoJuU12T4gXA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81109af6-2f99-5b3f-a3eb-b5fc208cd4da</td>\n",
       "      <td>[0.01485756, -0.06664991, -0.013571257, 0.0232...</td>\n",
       "      <td>Yes, even if you don't register, you're still ...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - Can I still join the course after the...</td>\n",
       "      <td>1721153497.323018</td>\n",
       "      <td>Zs34+PWIxmxARw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d1faac1c-a142-5cef-a493-dc392c68bcfb</td>\n",
       "      <td>[-0.023312058, -0.09461489, 0.05636162, -0.001...</td>\n",
       "      <td>You don't need it. You're accepted. You can al...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - I have registered for the Data Engine...</td>\n",
       "      <td>1721153497.323018</td>\n",
       "      <td>UkYTdZP1VbP8Kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a4190429-38b8-5994-8a0f-a7cee5b293d0</td>\n",
       "      <td>[0.026537674, -0.017796598, 0.0021156033, 0.00...</td>\n",
       "      <td>You can start by installing and setting up all...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - What can I do before the course starts?</td>\n",
       "      <td>1721153497.323018</td>\n",
       "      <td>jSDNTO0+ADbT5g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>7e5bcc2a-78ff-5b1e-908d-82db97e08b3a</td>\n",
       "      <td>[0.016619347, -0.033603188, -0.09334718, -0.02...</td>\n",
       "      <td>Problem description\\nThis is the step in the c...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Github actions: Permission denied error when e...</td>\n",
       "      <td>1721153497.323018</td>\n",
       "      <td>KIIPH7NoN0UumA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>628b7201-412d-5ed7-a264-ae06b0d3a522</td>\n",
       "      <td>[0.026872864, -0.0019948678, 0.008369055, -0.0...</td>\n",
       "      <td>Problem description\\nWhen a docker-compose fil...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Managing Multiple Docker Containers with docke...</td>\n",
       "      <td>1721153497.323018</td>\n",
       "      <td>W5Kx/1j92ul1EQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>ace2a7c9-53dc-5757-b306-52052eafdce9</td>\n",
       "      <td>[0.035137653, 0.05626561, 0.02442844, -0.06512...</td>\n",
       "      <td>Problem description\\nIf you are having problem...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>AWS regions need to match docker-compose</td>\n",
       "      <td>1721153497.323018</td>\n",
       "      <td>xOZcqQ5QX7FAqQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>b0bca538-93ec-511d-a5fa-806aa7c84156</td>\n",
       "      <td>[0.033809807, -0.003121895, 0.0017485361, 0.01...</td>\n",
       "      <td>Problem description\\nPre-commit command was fa...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Isort Pre-commit</td>\n",
       "      <td>1721153497.323018</td>\n",
       "      <td>7K8G/12zvF0TQw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>ed3cafb0-0b5c-55d6-961c-c3edea0d9b2d</td>\n",
       "      <td>[-0.00075233896, 0.0042315237, 0.0025023415, -...</td>\n",
       "      <td>Problem description\\nInfrastructure created in...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>How to destroy infrastructure created via GitH...</td>\n",
       "      <td>1721153497.323018</td>\n",
       "      <td>et4eNtnmNOmSJA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>948 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id__  \\\n",
       "0    9b6b8e02-7473-529b-8b65-5b90a44c4ccc   \n",
       "1    ff3d1217-a66e-5744-8e2e-ffb044b8cd12   \n",
       "2    81109af6-2f99-5b3f-a3eb-b5fc208cd4da   \n",
       "3    d1faac1c-a142-5cef-a493-dc392c68bcfb   \n",
       "4    a4190429-38b8-5994-8a0f-a7cee5b293d0   \n",
       "..                                    ...   \n",
       "943  7e5bcc2a-78ff-5b1e-908d-82db97e08b3a   \n",
       "944  628b7201-412d-5ed7-a264-ae06b0d3a522   \n",
       "945  ace2a7c9-53dc-5757-b306-52052eafdce9   \n",
       "946  b0bca538-93ec-511d-a5fa-806aa7c84156   \n",
       "947  ed3cafb0-0b5c-55d6-961c-c3edea0d9b2d   \n",
       "\n",
       "                                              vector__  \\\n",
       "0    [-0.00035096044, -0.062014237, -0.03799987, 0....   \n",
       "1    [0.02001144, -0.011535538, 0.013017191, -0.002...   \n",
       "2    [0.01485756, -0.06664991, -0.013571257, 0.0232...   \n",
       "3    [-0.023312058, -0.09461489, 0.05636162, -0.001...   \n",
       "4    [0.026537674, -0.017796598, 0.0021156033, 0.00...   \n",
       "..                                                 ...   \n",
       "943  [0.016619347, -0.033603188, -0.09334718, -0.02...   \n",
       "944  [0.026872864, -0.0019948678, 0.008369055, -0.0...   \n",
       "945  [0.035137653, 0.05626561, 0.02442844, -0.06512...   \n",
       "946  [0.033809807, -0.003121895, 0.0017485361, 0.01...   \n",
       "947  [-0.00075233896, 0.0042315237, 0.0025023415, -...   \n",
       "\n",
       "                                                  text  \\\n",
       "0    The purpose of this document is to capture fre...   \n",
       "1    GitHub - DataTalksClub data-engineering-zoomca...   \n",
       "2    Yes, even if you don't register, you're still ...   \n",
       "3    You don't need it. You're accepted. You can al...   \n",
       "4    You can start by installing and setting up all...   \n",
       "..                                                 ...   \n",
       "943  Problem description\\nThis is the step in the c...   \n",
       "944  Problem description\\nWhen a docker-compose fil...   \n",
       "945  Problem description\\nIf you are having problem...   \n",
       "946  Problem description\\nPre-commit command was fa...   \n",
       "947  Problem description\\nInfrastructure created in...   \n",
       "\n",
       "                              section  \\\n",
       "0    General course-related questions   \n",
       "1    General course-related questions   \n",
       "2    General course-related questions   \n",
       "3    General course-related questions   \n",
       "4    General course-related questions   \n",
       "..                                ...   \n",
       "943          Module 6: Best practices   \n",
       "944          Module 6: Best practices   \n",
       "945          Module 6: Best practices   \n",
       "946          Module 6: Best practices   \n",
       "947          Module 6: Best practices   \n",
       "\n",
       "                                              question       _dlt_load_id  \\\n",
       "0                 Course - When will the course start?  1721153497.323018   \n",
       "1    Course - What are the prerequisites for this c...  1721153497.323018   \n",
       "2    Course - Can I still join the course after the...  1721153497.323018   \n",
       "3    Course - I have registered for the Data Engine...  1721153497.323018   \n",
       "4     Course - What can I do before the course starts?  1721153497.323018   \n",
       "..                                                 ...                ...   \n",
       "943  Github actions: Permission denied error when e...  1721153497.323018   \n",
       "944  Managing Multiple Docker Containers with docke...  1721153497.323018   \n",
       "945           AWS regions need to match docker-compose  1721153497.323018   \n",
       "946                                   Isort Pre-commit  1721153497.323018   \n",
       "947  How to destroy infrastructure created via GitH...  1721153497.323018   \n",
       "\n",
       "            _dlt_id  \n",
       "0    xDljAqW4Go+btw  \n",
       "1    lXnoJuU12T4gXA  \n",
       "2    Zs34+PWIxmxARw  \n",
       "3    UkYTdZP1VbP8Kg  \n",
       "4    jSDNTO0+ADbT5g  \n",
       "..              ...  \n",
       "943  KIIPH7NoN0UumA  \n",
       "944  W5Kx/1j92ul1EQ  \n",
       "945  xOZcqQ5QX7FAqQ  \n",
       "946  7K8G/12zvF0TQw  \n",
       "947  et4eNtnmNOmSJA  \n",
       "\n",
       "[948 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_table = db.open_table(\"qanda_embedded___documents\")\n",
    "\n",
    "db_table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an up-to-date RAG with dlt and LanceDB\n",
    "In this demo, we will be creating an LLM chat bot that has the latest knowledge of the employee handbook of a fictional company. We will be able to chat to it about specific policies like PTO, work from home etc.\n",
    "\n",
    "To build this, we would need to do three things:\n",
    "1. The company policies exist in a [Notion Page](https://dlthub.notion.site/Employee-handbook-669c2a1e04044465811c8ca22977685d). We will need to first extract the text from these pages.\n",
    "2. Once extracted, we will want to embed them into vectors and then store them in a vector database.\n",
    "3. This will allow us to create our RAG: a function that would accept a user question, match it to the information stored in the vector database, and then send the question + relevant information as input to the LLM.\n",
    "\n",
    "#### We will be using the following OSS tools for this:\n",
    "1. dlt for data ingestion:  \n",
    "     1. dlt can easily connect to any REST API source (like Notion)\n",
    "     2. It also has integrations with vector databases, like LanceDB.\n",
    "     3. It also allows to easily plug in functionality like incremental loading.\n",
    "2. LanceDB as a vector database:\n",
    "     1. LanceDB is an open-source vector database that is very easy to use and integrate into python workflows\n",
    "     2. It is in-process and serverless (like DuckDB), which makes querying and retreival very efficient\n",
    "3. Ollama for RAG:\n",
    "     1. Ollama is open-source and allows you to easily run LLMs locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Notion -> LanceDB pipeline using dlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install requirements\n",
    "\n",
    "To create a notion -> lancedb pipeline, we need to install:\n",
    "  1. dlt with lancedb extras\n",
    "  2. sentence-transformers: we need to use an embedding model to vectorize and store data inside LanceDB. For this we     choose the open-source model \"sentence-transformers/all-MiniLM-L6-v2\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a dlt project with rest_api source and lancedb destination\n",
    "We now create a dlt project using the command `dlt init <source> <destination>`.\n",
    "\n",
    "What is the dlt rest api source?\n",
    "\n",
    "It is a dlt source that allows you to connect to any REST API endpoint using a declarative configuration. You can:\n",
    "- pass the endpoints that you want to connect to,\n",
    "- define the relation between the endpoints\n",
    "- define how you want to handle pagination and authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up the init scripts in \u001b[1mhttps://github.com/dlt-hub/verified-sources.git\u001b[0m...\n",
      "Cloning and configuring a verified source \u001b[1mrest_api\u001b[0m (Generic API Source)\n",
      "Do you want to proceed? [Y/n]: \n",
      "Verified source \u001b[1mrest_api\u001b[0m was added to your project!\n",
      "* See the usage examples and code snippets to copy from \u001b[1mrest_api_pipeline.py\u001b[0m\n",
      "* Add credentials for \u001b[1mlancedb\u001b[0m and other secrets in \u001b[1m./.dlt/secrets.toml\u001b[0m\n",
      "* \u001b[1mrequirements.txt\u001b[0m was created. Install it with:\n",
      "pip3 install -r requirements.txt\n",
      "* Read \u001b[1mhttps://dlthub.com/docs/walkthroughs/create-a-pipeline\u001b[0m for more information\n",
      "yes: stdout: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!yes | dlt init rest_api lancedb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add API credentials\n",
    "\n",
    "To access APIs, databases, or any third-party applications, one might need to specify relevant credentials.\n",
    "\n",
    "With dlt, we can do it in two ways:\n",
    "1. Pass the credentials and any other sensitive information inside `.dlt/secrets.toml`\n",
    "  ```toml\n",
    "  [sources.rest_api.notion]\n",
    "  api_key = \"notion api key\"\n",
    "\n",
    "  [destination.lancedb]\n",
    "  embedding_model_provider = \"sentence-transformers\"\n",
    "  embedding_model = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "  [destination.lancedb.credentials]\n",
    "  uri = \".lancedb\"\n",
    "  api_key = \"api_key\"\n",
    "  embedding_model_provider_api_key = \"embedding_model_provider_api_key\"\n",
    "  ```\n",
    "2. Pass them as environment variables\n",
    "  ```python\n",
    "  import os\n",
    "  \n",
    "  os.environ[\"SOURCES__REST_API__NOTION__API_KEY\"] = \"notion api key\"\n",
    "\n",
    "  os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL_PROVIDER\"] = \"sentence-transformers\"\n",
    "  os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL\"] = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "  os.environ[\"DESTINATION__LANCEDB__CREDENTIALS__URI\"] = \".lancedb\"\n",
    "  os.environ[\"DESTINATION__LANCEDB__CREDENTIALS__API_KEY\"] = \"api_key\"\n",
    "  os.environ[\"DESTINATION__LANCEDB__CREDENTIALS__EMBEDDING_MODEL_PROVIDER_API_KEY\"] = \"embedding_model_provider_api_key\"\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be using option 2. It's not advisable to paste sensitive information like API keys inside the code, so instead we're going to include them inside the secrets tab in the side panel of the notebook. This will allow us to access the secret values from the notebook.\n",
    "\n",
    "Since we are using the OSS version of LanceDB and OSS embedding models, we only need to specify the API key for Notion.\n",
    "\n",
    "**Note**: You will need to copy the [notion API key](https://share.1password.com/s#ohRHKjRIGagH_7HzxHzieZViCefOUmodTs2vodixXdQ) into the secrets tab under the name `SOURCES__REST_API__NOTION__API_KEY`. Make sure to enable notebook access after pasting the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"SOURCES__REST_API__NOTION__API_KEY\"] = 's#ohRHKjRIGagH_7HzxHzieZViCefOUmodTs2vodixXdQ'\n",
    "\n",
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL_PROVIDER\"] = \"sentence-transformers\"\n",
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL\"] = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "os.environ[\"DESTINATION__LANCEDB__CREDENTIALS__URI\"] = \".lancedb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write the pipeline code\n",
    "\n",
    "Note: We first go over the code step by step before putting it into runnable cells\n",
    "\n",
    "Import necessary modules (run this cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from rest_api import RESTAPIConfig, rest_api_source\n",
    "\n",
    "from dlt.sources.helpers.rest_client.paginators import BasePaginator, JSONResponsePaginator\n",
    "from dlt.sources.helpers.requests import Response, Request\n",
    "\n",
    "from dlt.destinations.adapters import lancedb_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Configure the dlt rest api source to connect to and extract the relevant data out from the Notion REST API.\n",
    "\n",
    "  Our notion space has multiple pages and each page has multiple paragraphs (called blocks). To extract all this data from the Notion API, we would first need to get a list of all the page_ids (each page has a unique page_id), and then use the page_id to request the contents from the individual pages. Specifically:\n",
    "  1. We will first request the page_ids from the `/search` endpoint\n",
    "  2. And then using the returned page_ids, we will request the contents from the `/blocks/{page_id}/children` endpoint\n",
    "\n",
    "  With this in mind, we can configure the dlt notion rest api source as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Configure the dlt rest api source to connect to and extract the relevant data out from the Notion REST API.\n",
    "\n",
    "  Our notion space has multiple pages and each page has multiple paragraphs (called blocks). To extract all this data from the Notion API, we would first need to get a list of all the page_ids (each page has a unique page_id), and then use the page_id to request the contents from the individual pages. Specifically:\n",
    "  1. We will first request the page_ids from the `/search` endpoint\n",
    "  2. And then using the returned page_ids, we will request the contents from the `/blocks/{page_id}/children` endpoint\n",
    "\n",
    "  With this in mind, we can configure the dlt notion rest api source as follows:\n",
    "```python\n",
    "  RESTAPIConfig = {\n",
    "        \"client\": {\n",
    "            \"base_url\": \"https://api.notion.com/v1/\",\n",
    "            \"auth\": {\n",
    "                \"token\": dlt.secrets[\"sources.rest_api.notion.api_key\"]\n",
    "            },\n",
    "            \"headers\":{\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Notion-Version\": \"2022-06-28\"\n",
    "            }\n",
    "        },\n",
    "        \"resources\": [\n",
    "            {\n",
    "                \"name\": \"search\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"search\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"paginator\": PostBodyPaginator(),\n",
    "                    \"json\": {\n",
    "                        \"query\": \"workshop\",\n",
    "                        \"sort\": {\n",
    "                            \"direction\": \"ascending\",\n",
    "                            \"timestamp\": \"last_edited_time\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"data_selector\": \"results\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"page_content\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"blocks/{page_id}/children\",\n",
    "                    \"paginator\": JSONResponsePaginator(),\n",
    "                    \"params\": {\n",
    "                        \"page_id\": {\n",
    "                            \"type\": \"resolve\",\n",
    "                            \"resource\": \"search\",\n",
    "                            \"field\": \"id\"\n",
    "                        }\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "  1. `client`: Here we added our base url, headers, and authentication\n",
    "  2. `resources`: This is a list of endpoints that we wish to request data from (here: `/search` and `/blocks/{page_id}/children`)\n",
    "  3. [`/search`](https://developers.notion.com/reference/post-search) endpoint:\n",
    "      - The Notion API search endpoint allows us to filter pages based on the title. We can specify which pages we want returned based on the parameter \"query\". For example, if we'd like to return only those pages which has the word \"workshop\" in the title, then we would set `\"query\": \"workshop\"` in the json body.    \n",
    "      - As a response, it returns only page metadata (like page_id). Example response:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```json\n",
    "    {\n",
    "      \"object\": \"list\",\n",
    "      \"results\": [\n",
    "        {\n",
    "          \"object\": \"page\",\n",
    "          \"id\": \"954b67f9-3f87-41db-8874-23b92bbd31ee\",\n",
    "          \"created_time\": \"2022-07-06T19:30:00.000Z\",\n",
    "          \"last_edited_time\": \"2022-07-06T19:30:00.000Z\",\n",
    "          .\n",
    "          .\n",
    "          .\n",
    "      ],\n",
    "      \"next_cursor\": null,\n",
    "      \"has_more\": false,\n",
    "      \"type\": \"page_or_database\",\n",
    "      \"page_or_database\": {}\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - This is how we would define our endpoint configuration for `/search`:\n",
    "```python\n",
    "      {\n",
    "        \"name\": \"search\",\n",
    "        \"endpoint\": {\n",
    "            \"path\": \"search\",\n",
    "            \"method\": \"POST\",\n",
    "            \"paginator\": PostBodyPaginator(),\n",
    "            \"json\": {\n",
    "                \"query\": \"workshop\",\n",
    "                \"sort\": {\n",
    "                    \"direction\": \"ascending\",\n",
    "                    \"timestamp\": \"last_edited_time\"\n",
    "                }\n",
    "            },\n",
    "            \"data_selector\": \"results\"\n",
    "        }\n",
    "    },"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `paginator` allows us to specify the pagination strategy relevant for the API and the endpoint. (More on this later)\n",
    "- Since `/search` is a POST endpoint, we can include the json body inside the key `json`.\n",
    "- We don't need the whole JSON response, but only the contents inside the field \"results\". We filter this out by specifying `\"data_selector\": \"results\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [`blocks/{page_id}/children`](https://developers.notion.com/reference/get-block-children) endpoint:\n",
    "  - This is a GET point that returns a list of block objects (in our case, paragraphs) from a specific page.\n",
    "  - Since it accepts page_id as a parameter, we can pass this inside the key `params`\n",
    "  - We would like to be able to automatically fetch the page_ids returned from the `/search` endpoint and pass it as parameter into the endpoint `blocks/{page_id}/children`. We can do this by linking the two resources as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "{\n",
    "      \"name\": \"page_content\",\n",
    "      \"endpoint\": {\n",
    "          \"path\": \"blocks/{page_id}/children\",\n",
    "          \"paginator\": JSONResponsePaginator(),\n",
    "          \"params\": {\n",
    "              \"page_id\": {\n",
    "                  \"type\": \"resolve\",\n",
    "                  \"resource\": \"search\",\n",
    "                  \"field\": \"id\"\n",
    "              }\n",
    "          },\n",
    "      }\n",
    "}\n",
    "```\n",
    "- By specifying `\"type\":\"resolve\"`, we are letting dlt know that this parameter needs to be resolved from the parent resource `\"search\"` using the field `\"id\"`, which corresponds to the page id in the response of `/search`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
